# NLU 와 NLG 

## NLU(Natural Language Understanding) | 자연어 이해

- NLU는 자연어 처리(NLP)의 하위 분야
- 컴퓨터 소프트웨어를 활용하여 음성이나 텍스트의 입력된 문장형태를 이해하는것

## NLG(Natural Language Generation)

- 컴퓨터가 자연어 텍스트를 생성하여 인간이 자연스럽게 소통하는 방식을 모방
- 인간과 유사한 텍스트나 음성 생성

## 자연어 처리의 어려움

- 문맥의 따른 모호성
- 표현의 중의성
- 규칙의 예외성

## 교착어, 굴절어, 고립어

- 교착어
  - 한국어,일본어,몽골어
    - 어간에 접사가 붙어 단어를 이루고 의미와 문법적 기능이 정해짐
- 굴절어
  - 라틴어,독일어,러시아어
    - 단어의 형태가 변하면서 문법적 기능이 변하는 언어
- 고립어
  - 영어,중국어
    - 단어의 형태가 변하지 않고 단어의 위치나 문맥으로 문법적 기능을 구분하는 언어

## 한국어에서 자연처리가 어려운 이유

- 교착어
  - 한국어는 교착어로 단어의 형태가 변하면서 문법적인 기능이 변하는 언어
    - ex) '나는' -> '나를' -> '나에게' -> '나로'
    - ex) '먹다' -> '먹는다' -> '먹었다' -> '먹으려면'
    - 타 언어에 비해 같은 단어라도 다양한 조합이 존재
- 단어 순서 및 주어생략
    - 한국어는 주어를 생략하는 경우가 많고 단어의 순서가 중요하지 않음
    - ex) '나는 밥을 먹었다' -> '밥을 먹었다' -> '먹었다'
    - ex) '나는 밥을 먹었다' -> '먹었다' -> '나는'
- 띄어쓰기
  - 한국어는 띄어쓰기가 제대로 지켜지지 않는 경우가 많음
    - ex) '나는 밥을 먹었다' -> '나는밥을먹었다'
    - ex) '나는 밥을 먹었다' -> '나는 밥을먹었다'
  - 정제단계 중 하나인 '분절' 단계에서 혼란 발생

## 언어학의 접근 방법

- 규칙기반 접근:
  - 언어학적 지식을 활용하여 문법적인 규칙을 만들어 자연어 처리
- 통계기반 접근:
  - 대량의 데이터를 활용하여 자연어 처리
  - 전자화된 텍스트(코퍼스)의 분석을 통해 얻어진 언어 단위의 분포와 빈도에 관한 정보 이용
- 신경망 기반 접근:
  - 인공신경망을 활용하여 자연어 처리

## 전통적인 자연언어 처리 파이프라인

1. Document
2. Pre-process
3. Tokenize, Sentence, Split
4. Part of speech tagger
5. Chunker
6. Class Matching
7. Querying
8. Post-process
9. Structured Data

## 음절, 형태소, 어절, 품사

- 음절
  - 하나의 덩어리로 여겨지는 최소 음운 단위
- 형태소
  - 의미를 가지는 최소 단위
  - 자립형태소, 의존형태소로 구분
- 어절
  - 띄어쓰기 단위
- 품사
  - 단어의 문법적 기능을 나타내는 분류
  - 품사를 분석해 구분하는 작업으 POS(Part of Speech) Tagging이라고 함
- 품사의 구분
  - 역할에 따라
    - 체언, 용언, 수식언, 관계언, 독립언
  - 의미에 따라
    - 명사, 대명사, 수사, 관형사, 부사,조사,감탄사,동사,형용사
  - 형태에 따라
    - 가변어, 불변어

## 형태론(morphology)

- 언어에서 의미를 갖는 가장 기본단위인 형태소를 분석
- 형태소 간의 상관관계를 규명하는 학문

## 형태소(morpheme)

- 의미를 가지는 최소 단위
- 의미 혹은 문법적 기능의 최소단위
# 형태소(Morpheme)

## 형태소의 종류

### 자립성 여부
- 자립 형태소
  - 의미: 독립적으로 사용될 수 있는 형태소로, 문장에서 독자적인 의미를 가짐
  - 종류: 명사, 동사, 형용사, 부사, 감탄사
  - 예시: 나무, 먹다, 아름답다, 빨리, 아!

- 의존 형태소
  - 의미: 독립적으로 사용될 수 없으며, 다른 형태소와 결합하여 의미를 형성함
  - 종류: 조사, 접사, 어미
  - 예시: -은, -이, -고, -었

### 의미적 분류
- 실질 형태소(어휘 형태소)
  - 의미: 사물이나 개념을 직접 지칭하며, 실질적인 의미를 가짐
  - 종류: 명사, 동사, 형용사, 부사
  - 예시: 책, 달리다, 크다, 천천히

- 형식 형태소(문법 형태소)
  - 의미: 문법적 관계를 나타내는 역할을 하며 독립적인 의미가 없음
  - 종류: 조사, 접사, 어미
  - 예시: -은, -이, -고, -었

---

## 형태와 이형태

- 형태(Form): 의미를 가지는 최소 단위로, 변하지 않는 기본 단위
- 이형태(Allomorph): 같은 의미를 가지지만 문법적 환경에 따라 형태가 변형되는 경우 (예: ‘-이’와 ‘-가’)

---

# 통사론(Syntax)

- 문장의 구조와 형식을 연구하는 언어학의 한 분야

## 구조적 모호성(Structural Ambiguity)

- 동일한 표층 구조가 여러 개의 심층 구조를 가질 수 있는 현상
  - 예: "나는 그녀의 사진을 좋아한다." (그녀가 찍힌 사진 vs. 그녀의 소유물)

## 반복(Recursion)

- 문법 규칙이 자기 자신을 호출할 수 있는 성질로, 무한한 문장 생성이 가능함
  - 예: "그는 그녀가 온 것을 몰랐다." → 문장 안에 문장이 중첩됨

## 구 구조규칙(Phrase Structure Rule)

- 문장의 기본 구조를 정의하는 규칙
  - 예: S → NP VP (문장은 명사구와 동사구로 구성됨)

## 어휘규칙(Lexical Rule)

- 특정한 어휘가 문장에서 어떻게 사용될 수 있는지를 기술하는 규칙

## 변형규칙(Transformation Rules)

- 문장의 구조를 변형시키는 규칙
  - 예: 능동태 → 수동태 변형 ("철수가 책을 읽었다." → "책이 철수에 의해 읽혔다.")

---

# 의미론(Semantics)

- 개념적 의미(Conceptual Meaning): 단어의 기본적인 의미 요소
- 연상적 의미(Associative Meaning): 개인적, 사회적 맥락에 따른 의미적 확장

## 의미자질(Semantic Features)

- 단어의 의미적 속성을 분석하여 의미를 구성하는 최소 단위로 구분
  - 예: ‘사람’ → [+생명체, +이성적 존재]

## 의미역(Semantic Roles)

- 문장에서 각 성분이 담당하는 의미적 역할
  - 예: 주체(Agent), 대상(Patient), 도구(Instrument), 장소(Location)

## 의미 관계(Semantic Relations)

- 상하관계(Hyponymy): 상위어와 하위어의 관계 (예: ‘과일’ - ‘사과’)
- 동음 이철어(Homophones): 동일한 발음을 가지지만 의미가 다른 단어 (예: 배(boat) vs. 배(stomach))
- 동음 이어의(Homonyms): 동일한 철자와 발음을 가지지만 의미가 완전히 다른 단어 (예: 밤(night) vs. 밤(chestnut))
- 다의어(Polysemy): 하나의 단어가 여러 관련된 의미를 가짐 (예: 손(hand) → 신체부위 vs. 도움)
- 연어(Collocation): 특정 단어들이 함께 자주 사용되는 패턴 (예: "높은 산", "깊은 바다")

---

# 화용론(Pragmatics)

- 문맥과 사회적 요인에 따라 의미가 어떻게 변화하는지를 연구하는 분야

---

# 자연언어처리(NLP)에서의 언어학

## 주요 분석 기법
- 토큰화(Tokenization): 텍스트를 의미 단위로 나누는 과정
- 품사 태깅(POS Tagging): 각 단어에 품사 정보를 부착
- 구문 분석(Syntax Parsing): 문장의 문법적 구조 분석
- 의미 분석(Semantic Analysis): 문장의 의미를 해석
- 개체명 인식(NER): 고유명사(사람, 장소, 조직 등) 식별
- 문법 교정(GEC): 문장의 문법 오류 수정
- 의존 구문 분석(Dependency Parsing): 단어 간의 종속 관계를 분석

## BERT와 언어 구조
- BERT는 문맥을 고려하여 단어 간 관계를 학습하며, 다층적인 의미 표현을 가능하게 함
- LIMIT-BERT: 기존 BERT 모델에 언어학적 지식을 추가하여 성능을 향상시킨 모델

---

# 전처리(Preprocessing)

## 주요 기법
- HTML 태그, 특수문자, 이모티콘 제거: 비언어적 요소 제거
- 정규표현식(Regular Expression): 패턴 기반 텍스트 정제
- 불용어(Stopword) 제거: 자주 등장하지만 의미가 적은 단어 필터링
- 어간추출(Stemming) vs. 표제어 추출(Lemmatization):
  - 어간추출: 단어의 기본형을 찾기 위해 단순 변환 (예: ‘running’ → ‘run’)
  - 표제어 추출: 문맥을 고려하여 정확한 기본형을 반환 (예: ‘better’ → ‘good’)

## 주요 라이브러리
- KoNLPy: 한국어 형태소 분석 라이브러리
- NLTK: 영어 자연어 처리 도구

## 토큰화(Tokenization) 주의사항
- 하나의 단어가 다양한 의미를 가질 수 있음
- 문맥에 따라 다른 품사로 해석될 가능성이 존재함

# --- 25.03.07(금)

## Lemmatization (표제어 추출)
- 단어의 원형을 찾아주는 과정
- 예: "running" → "run"
- 어간추출(stemming)과 달리 문법적으로 정확한 원형을 유지
- 적용 예시: 검색 엔진, 텍스트 정규화

## Levenshtein Distance (편집 거리)
- 두 문자열을 서로 같게 만들기 위해 필요한 최소한의 편집 작업(삽입, 삭제, 치환)의 수를 의미
- 편집 작업:
  - 삽입: 한 문자열에 문자를 추가하는 작업
  - 삭제: 문자열에서 문자를 제거하는 작업
  - 치환: 한 문자열의 문자를 다른 문자로 바꾸는 작업
- 응용 분야:
  - 오타 수정: 문서 작성시 단어 간의 유사성을 판단하여 오타를 교정
  - 자연어 처리: 문장 유사도 계산, 텍스트 검색 및 추천 시스템
  - 생물정보학: DNA, RNA, 단백질 서열의 유사성 분석

## 형태소 분석기
- 형태소(Morpheme): 의미를 가지는 가장 작은 단위
  - 자립형 형태소: 혼자서도 쓰일 수 있는 형태소 (예: 명사, 동사, 형용사)
  - 의존형 형태소: 혼자 쓰일 수 없고 다른 형태소와 결합해야 하는 형태소 (예: 조사, 접사, 어미)
- 형태소 분석(Morphological Analysis):
  - 문장을 형태소 단위로 나누고 각 형태소의 품사를 분석하는 과정
- 주요 기능
  - 토큰화(Tokenization): 문장을 형태소 단위로 분할
  - 품사 태깅(POS Tagging): 각 형태소의 품사를 판별
  - 어근 추출(Lemmatization, Stemming): 변형된 단어를 원형으로 변환
  - 어휘 정규화(Normalization): 형태가 다른 단어들을 같은 의미로 통일
- 주요 알고리즘 및 기법
  - 사전 기반 분석: 미리 구축된 형태소 사전을 사용하여 분석 (예: MeCab, 한나눔)
  - 규칙 기반 분석: 문법 규칙을 활용하여 문장을 분석 (예: 의존 구문 분석)
  - 통계 기반 분석: 머신러닝 및 딥러닝을 활용하여 확률적으로 분석 (예: Word2Vec, BERT)
- 한국어 형태소 분석기의 특징과 난이도
  - 한국어는 교착어(agglutinative language)로, 어미 변화가 많아 분석이 어렵다.
  - 띄어쓰기가 불규칙하여 문장 분리가 어렵다. (예: "오늘밤" vs. "오늘 밤")
  - 하나의 단어가 여러 의미를 가질 수 있어 품사 중의성(ambiguity) 문제가 발생한다.
- 분석기 특징
  - MeCab	속도가 빠르고, 일본어 기반이지만 한국어용 사전 추가 가능
  - KoNLPy	다양한 형태소 분석기 제공 (Okt, Komoran, Hannanum, Kkma 등)
  - Khaiii	카카오에서 개발한 딥러닝 기반 한국어 형태소 분석기
  - ETRI	한국전자통신연구원(ETRI)에서 개발한 형태소 분석 API

## 품사 태깅(POS Tagging)
- 품사 태깅(Part-of-Speech Tagging, POS Tagging)은 문장에서 각 단어(형태소)의 품사를 분석하여 태그를 부착하는 과정
- 개념 및 정의
  - 품사(POS, Part of Speech): 단어가 문장에서 수행하는 역할을 의미함. (예: 명사, 동사, 조사 등)
  - 품사 태깅(POS Tagging): 문장에서 단어별 품사를 자동으로 판별하여 태그를 부착하는 과정
  - 활용 목적: 문법 분석, 문장 구조 이해, 자연어 처리(NLP) 모델 학습
- 주요 기능
  - 토큰화(Tokenization): 문장을 단어(형태소) 단위로 분리
  - 품사 판별(POS Classification): 단어별 품사를 분석하여 태깅
  - 의미 해석(Ambiguity Resolution): 동일한 단어가 문맥에 따라 다른 품사를 가질 경우 적절한 품사 선택
- 주요 품사 태깅 기법
  - 사전 기반 분석(Dictionary-based)
    - 미리 구축된 품사 사전을 활용하여 단어별 품사를 부착
    - 장점: 빠르고 간단함
    - 단점: 신조어, 문맥에 따른 중의성 해결이 어려움
    - 규칙 기반 분석(Rule-based)
- 문법 규칙(예: 조사 다음에는 명사가 올 확률이 낮음)을 활용하여 품사 결정
  - 장점: 언어적 특성을 반영 가능
  - 단점: 규칙을 수동으로 설계해야 하며 복잡한 문장에서는 오류 발생 가능
- 통계 기반 모델(Statistical-based)
  - HMM(Hidden Markov Model) 등 확률 기반 모델을 활용하여 문맥 정보를 반영
  - 장점: 문맥 고려 가능, 자동 학습 가능
  - 단점: 충분한 학습 데이터가 필요
- 딥러닝 기반 모델(Deep Learning-based)
  - BiLSTM, Transformer, BERT 등을 활용하여 문맥을 깊이 이해하는 방식
  - 장점: 높은 정확도, 문맥 고려 가능
  - 단점: 많은 데이터와 연산 자원이 필요


## 규칙 기반 형태소 분석 및 품사 태깅
- 규칙 기반(Rule-Based) 형태소 분석과 품사 태깅은 언어적 규칙을 활용하여 단어를 분석하고 품사를 판별하는 방법
- 사전 기반 방식과 함께 전통적인 자연어 처리(NLP) 기법 중 하나이며, 규칙을 직접 설계하여 분석을 수행
- 개념 및 정의
  - 형태소 분석(Morphological Analysis)
  - 문장에서 형태소(의미를 가지는 최소 단위)를 추출하는 과정
  - 어근과 접사(예: '먹었다' → ['먹', '었', '다'])를 분리
- 품사 태깅(POS Tagging)
  - 형태소 단위로 분리된 단어에 대해 문맥에 따라 적절한 품사를 부착하는 과정
  - 예: "나는 학교에 간다" → ('나', 대명사), ('는', 조사), ('학교', 명사), ('에', 조사), ('간다', 동사)
- 규칙 기반 분석(Rule-Based Analysis)
  - 문법적 규칙을 정의하여 형태소 분할 및 품사 태깅 수행
  - "어미가 '-다'로 끝나면 동사일 가능성이 높음" 등의 규칙 활용
- 규칙 기반 방법론
  - 규칙 기반 형태소 분석 및 품사 태깅은 언어적 규칙을 활용하여 문장을 분석하는 방식으로, 다음과 같은 방법이 사용됩니다.
  - 접사 및 어미 분석
    - 한국어는 어미(접미사)가 많아, 어미 변형을 고려한 규칙 설계가 필요합니다.
    - 예시:
    - 동사(Verb): "먹었다" → "먹 + 었다(과거형 어미)"
    - 형용사(Adjective): "예쁘다" → "예쁘 + 다(어미)"
    - 명사(Noun): "학생들" → "학생 + 들(복수형 접미사)"
  - 조사(Particle) 판별
    - 조사(예: '은', '를', '이', '에')는 명사 뒤에 오는 경우가 많음.
    - 예시:
    - "학교에 간다" → "학교(N) + 에(Josa)"
  - 문맥 기반 규칙 적용
    - 품사 중의성 해결: 같은 단어라도 문맥에 따라 품사가 달라짐.
    - 예시:
    - "나는 사과를 먹었다" (사과 = 명사)
    - "그의 사과를 받아들였다" (사과 = 동사)
  - 어휘 기반 사전 활용
    - 기본적인 명사, 동사, 형용사 등의 목록을 사전에 포함하여 정확도를 높임.
- 규칙 기반 분석의 장점과 단점
  - 장점
    - 설명 가능성(Explainability): 규칙이 명확하여 사람이 직접 해석 가능
    - 언어적 지식 반영 가능: 특정 언어의 문법 규칙을 직접 반영 가능
    - 사전 기반 분석과 결합 가능: 품사 사전과 함께 사용하여 성능 향상
  - 단점
    - 규칙 설계의 복잡성: 언어적 규칙이 많아지고 복잡해질수록 유지보수가 어려움
    - 예외 처리의 어려움: 신조어, 문맥 변화 등 예외적인 경우를 처리하기 어려움
    - 일반화 한계: 새로운 문장에서 규칙을 적용하기 어려울 수 있음

## 통계 기반 형태소 분석 및 품사 태깅
- 통계 기반(Statistical) 형태소 분석과 품사 태깅은 기계 학습을 활용하여 형태소를 분석하고 품사를 자동으로 태깅하는 방식
- 기존의 규칙 기반 분석과 달리, 대량의 학습 데이터를 바탕으로 확률적으로 형태소를 분석하는 것이 특징
- 개념 및 정의
  - 형태소 분석(Morphological Analysis)
    - 문장에서 형태소(의미를 가지는 최소 단위)를 추출하는 과정
    - 예: "먹었다" → "먹(VV) + 었(EP) + 다(EF)"
  - 품사 태깅(POS Tagging)
    - 단어에 확률적으로 적절한 품사를 부착하는 과정
    - 예: "나는 학교에 간다" → ('나', NP), ('는', JX), ('학교', NNG), ('에', JKB), ('간다', VV+EF)
  - 통계 기반 분석(Statistical Analysis)
    - 대량의 말뭉치(corpus)를 활용하여 문맥에서 형태소 및 품사의 확률을 학습하는 방식
    - 기계 학습 및 확률 모델을 활용하여 규칙을 직접 정의하지 않아도 학습이 가능함
- 주요 기법
  - Hidden Markov Model (HMM)
    - HMM 기반 품사 태깅은 문장에서 단어의 품사를 최대 확률(최적의 태그 시퀀스)로 예측하는 방식
    - Markov 가정: 현재 단어의 품사는 이전 단어의 품사에 영향을 받음
    - 한계: 문맥을 길게 고려하기 어려움
  - Conditional Random Fields (CRF)
    - HMM보다 문맥 정보를 더 정교하게 반영하는 조건부 확률 모델
    - 문장에서 단어 간 연관성을 학습하여 품사 태깅을 수행
    - 예: "나는 학교에 간다" → '학교' 다음에는 '에'가 조사로 나올 확률이 높음
    - 장점: 문맥 의존성이 강한 한국어에 적합
    - 단점: 학습 데이터가 많아야 성능이 향상됨
  - Neural Networks (딥러닝 기반)
    - RNN, BiLSTM, Transformer 등의 딥러닝 모델을 활용하여 형태소 분석 및 품사 태깅 수행
    - 대표적인 모델
      - BiLSTM-CRF: 문장의 양방향 문맥을 반영하여 높은 성능 달성
      - BERT 기반 모델: 한국어 문장에서 품사 예측 성능이 우수함
- 통계 기반 분석의 장점과 단점
  - 장점
    - 규칙 없이 자동 분석 가능: 데이터만 있다면 자동 학습 가능
    - 문맥 고려 가능: HMM, CRF, 딥러닝 모델은 문맥에 따라 품사 예측 가능
    - 다양한 언어 적용 가능: 언어별 특수 규칙 없이 데이터만 있으면 적용 가능
  - 단점
    - 학습 데이터가 많아야 함: 충분한 학습 데이터가 없으면 정확도가 낮음
    - 훈련 시간이 길다: 특히 딥러닝 기반 모델은 학습 시간이 오래 걸림
    - 설명 가능성 낮음: 딥러닝 기반 모델은 결정 과정이 불투명함
- 대표적인 통계 기반 품사 태깅 도구
  - MeCab: 일본어 기반이지만 한국어 지원, 빠른 처리 속도
  - KoNLPy: 여러 한국어 분석기(Okt, Komoran, Kkma) 포함
  - CRFSuite: CRF 기반 품사 태깅 가능
  - KoBERT: 딥러닝 기반으로 문맥을 깊이 반영한 품사 태깅 가능


## 딥러닝 기반 형태소 분석 및 품사 태깅
- 딥러닝 기반 형태소 분석과 품사 태깅은 신경망 모델을 활용하여 문맥을 반영한 형태소 분석 및 품사 예측을 수행하는 기법
- 기존의 규칙 기반(Rule-based), 통계 기반(Statistical) 방식과 달리 데이터를 학습하여 자동으로 패턴을 추론하는 것이 특징
- 개념 및 정의
  - 형태소 분석(Morphological Analysis)
    - 문장에서 의미를 가지는 최소 단위(형태소)를 추출하는 과정
    - 예: "먹었다" → "먹(VV) + 었(EP) + 다(EF)"
  - 품사 태깅(POS Tagging)
    - 단어별로 문맥을 고려하여 가장 적절한 품사를 부착하는 과정
    - 예: "나는 학교에 간다" → ('나', NP), ('는', JX), ('학교', NNG), ('에', JKB), ('간다', VV+EF)
  - 딥러닝 기반 분석(Deep Learning-based Analysis)
    - 기존의 규칙을 직접 정의하지 않고, 신경망 모델이 데이터를 학습하여 패턴을 자동으로 학습
    - 문맥 정보를 깊이 반영하여 품사 중의성(Ambiguity) 해결 능력이 뛰어남
    - RNN, LSTM, BiLSTM, Transformer(BERT) 등의 모델을 활용
- 주요 딥러닝 모델
  - RNN (Recurrent Neural Network)
    - 순차적인 데이터(자연어)에 적합한 신경망 구조
    - 단점: 긴 문장에서 장기 의존성 문제 발생 (이전 단어의 정보가 뒷부분에서 희미해짐)
  - LSTM (Long Short-Term Memory)
    - RNN의 단기 기억 문제를 해결하기 위한 구조
    - 장점: 긴 문맥을 기억하여 품사 태깅 정확도 향상
    - 단점: 학습 속도가 느리고, 계산량이 많음
  - BiLSTM (Bidirectional LSTM)
    - 문장의 앞뒤 문맥을 모두 고려하는 양방향 LSTM 모델
    - 한국어와 같이 어순이 자유로운 언어에서 효과적
    - 많은 NLP 태스크에서 높은 성능을 보임
  - BiLSTM-CRF (Conditional Random Field)
    - BiLSTM의 문맥 학습 능력 + CRF의 의존성 처리 능력을 결합한 모델
    - 한국어 품사 태깅에서 많이 사용됨
    - 예제 구조
      - "학교에 간다" → BiLSTM이 단어별 확률 예측
      - CRF가 문맥을 고려하여 최적의 품사 시퀀스를 생성
  - Transformer & BERT
    - Transformer: 
      - 기존 RNN/LSTM을 대체하는 모델로, Self-Attention을 사용하여 문맥을 효과적으로 학습
    - BERT (Bidirectional Encoder Representations from Transformers):
      - 문장의 양방향 문맥을 반영하여 품사 태깅 정확도 향상
      - 사전 학습된 모델을 사용하여 적은 데이터로도 높은 성능 가능
      - 한국어 전용 모델: KoBERT, KcBERT, SKT-KoBERT 등이 있음
- 딥러닝 기반 품사 태깅의 장점과 단점
  - 장점
    - 문맥을 깊이 반영: BiLSTM, Transformer 기반 모델이 문맥을 고려하여 분석
    - 학습 데이터만 있으면 자동화 가능: 규칙을 직접 정의하지 않아도 됨
    - 중의성 해결 가능: 같은 단어라도 문맥에 따라 다른 품사로 태깅 가능
  - 단점
    - 대량의 학습 데이터 필요: 딥러닝 모델은 많은 데이터가 필요함
    - 훈련 시간이 길다: 특히 Transformer 기반 모델은 연산량이 많음
    - 설명 가능성이 낮음: 모델이 어떻게 결정을 내렸는지 해석하기 어려움

## HMM: Hidden Markov Model
- Hidden Markov Model(HMM)은 관찰된 데이터(Observable Data) 뒤에 숨겨진(hidden) 상태(State)들을 확률적으로 모델링하는 기법
- HMM의 개념 및 정의
  - 마르코프 모델(Markov Model)
  - 현재 상태가 다음 상태를 결정하는 확률 모델
  - Markov Assumption (마르코프 가정):
    - 다음 상태는 이전 상태에만 의존하며, 더 과거의 상태에는 영향을 받지 않음.
    - 수식:
      - $ P(X_t | X_{t-1}, X_{t-2}, ..., X_1) = P(X_t | X_{t-1}) $
  - 은닉 마르코프 모델(HMM)
    - 관찰할 수 없는 숨겨진 상태(Hidden State)와 관찰 가능한 출력(Observed Data)을 포함한 마르코프 모델
    - 예를 들어, 품사 태깅(POS Tagging) 문제에서:
      - 숨겨진 상태(Hidden State) → 품사(명사, 동사 등)
      - 관찰된 데이터(Observable Sequence) → 실제 단어(나는, 학교에, 간다 등)
- 히든 마르코프 모델(HMM)의 핵심 요소
- HMM은 5가지 요소로 구성됩니다.

$$ \lambda = (A, B, \pi, S, O) $$

| 요소 | 설명 | 예시 (품사 태깅) |
|------|------|----------------|
| S (State Set) | 숨겨진 상태(품사) | {NNG(명사), VV(동사), JKB(조사)} |
| O (Observation Set) | 관찰 가능한 데이터(단어) | {나는, 학교에, 간다} |
| A (Transition Probability) | 상태 간 전이 확률 | $P(동사 \mid 명사)$ |
| B (Emission Probability) | 특정 상태에서 특정 단어가 나타날 확률 | $P(\text{"학교에"} \mid JKB)$ |
| π (Initial Probability) | 시작 확률(첫 상태의 확률) | $P(명사에서 시작할 확률)$ |

- HMM 기반 품사 태깅의 장점과 단점
  - 장점
    - 확률적 접근 방식: 규칙을 직접 정의하지 않고 데이터 기반 학습 가능
    - 문맥을 고려한 태깅: 단순한 사전 기반 방식보다 문맥을 반영 가능
    - 비터비 알고리즘 활용 가능: 최적의 품사 시퀀스를 찾을 수 있음
  - 단점
    - Markov Assumption의 한계: 문장의 이전 한 단어만 고려하여 더 긴 문맥 반영이 어려움
    - 동일한 확률 문제: 단어가 학습 데이터에 없을 경우(OOV 문제) 품사 태깅이 어려움
    - 딥러닝 대비 성능 부족: 최근 BiLSTM, BERT 기반 모델이 더 높은 정확도를 제공
- HMM과 다른 모델 비교

| 모델 | 장점 | 단점 |
|------|------|------|
| 규칙 기반 (Rule-Based) | 직접 규칙을 정의하여 해석 가능 | 예외 처리 어려움, 신조어 대응 어려움 |
| HMM (은닉 마르코프 모델) | 문맥을 반영하여 확률적 태깅 가능 | 긴 문맥 반영 어려움, OOV 문제 |
| CRF (Conditional Random Fields) | 문맥 정보 반영 능력 향상 | 학습 데이터가 많아야 함 |
| BiLSTM-CRF | 딥러닝 기반으로 높은 정확도 | 학습 데이터가 많아야 하고 계산량 많음 |
| BERT 기반 모델 | 문맥을 깊이 반영하여 최고 성능 | 연산량 많고 학습이 어려움 |


- 결론
  - HMM은 확률적 방법을 사용하여 형태소 분석과 품사 태깅을 수행하는 전통적인 모델
  - Viterbi 알고리즘을 통해 최적의 품사 태깅 시퀀스를 예측 가능([참고](https://lucy-the-marketer.kr/ko/viterbi-algorithm/))
  - 최근에는 BiLSTM-CRF, Transformer(BERT) 기반 모델이 더 높은 성능을 보이지만, HMM은 여전히 기본적인 NLP 기법으로 활용됨
  - 특히 학습 데이터가 적을 때 HMM이 유용할 수 있으며, 확률적 모델링의 개념을 이해하는 데 중요한 기법


## CRF: Conditional Random Field
- Conditional Random Field (CRF, 조건부 확률장)는 순차 데이터(sequence data)에서 문맥(Context)을 고려하여 태깅하는 지도 학습 기반의 확률 모델
- 품사 태깅(POS Tagging), 개체명 인식(NER), 형태소 분석 등 자연어 처리(NLP)에서 HMM(은닉 마르코프 모델)보다 더 정교한 태깅이 가능

- HMM vs. CRF 비교

| 모델 | 특징 | 한계 |
|------|------|------|
| HMM (Hidden Markov Model) | 이전 한 개의 상태만 고려하여 품사 태깅 | 긴 문맥을 고려하기 어려움 |
| CRF (Conditional Random Field) | 문맥 전체를 활용하여 태깅 | 학습 데이터가 많아야 함 |

- CRF는 HMM과 달리 현재 단어뿐만 아니라 문장의 전체적인 문맥을 반영할 수 있는 강점을 가집니다.

- CRF의 핵심 개념
- CRF의 주요 요소
  - CRF는 아래 세 가지 요소를 기반으로 동작합니다.

| 요소 | 설명 |
|------|------|
| X (Observation Sequence) | 관찰된 데이터 (예: 단어 시퀀스) |
| Y (Label Sequence) | 예측해야 할 라벨 (예: 품사 태그) |
| Feature Function (특징 함수, f(X, Y)) | 문맥적 특징을 반영하는 함수 |

- CRF는 X(입력 데이터)와 Y(출력 라벨) 간의 관계를 학습하며,  
- 특징 함수(Feature Function)를 활용하여 문맥 정보를 반영한 태깅을 수행합니다.

- CRF의 동작 과정
  - 입력 데이터(X) 준비: 문장을 단어 단위로 분할
  - 특징 추출(Feature Extraction): 단어의 형태, 주변 단어 등을 기반으로 특징 생성
  - 모델 학습(Training): 확률적으로 가장 적절한 가중치 학습
  - 예측(Inference): 새로운 문장에서 가장 적절한 태깅 수행
- CRF 기반 품사 태깅의 장점과 단점
  - 장점
    - 문맥을 반영한 품사 태깅 가능: 앞뒤 단어를 고려하여 더 정확한 태깅 수행
    - HMM보다 강력한 모델: HMM은 이전 한 개의 상태만 반영하지만, CRF는 전체 문맥을 반영
    - 의존 관계 처리 가능: 특정 품사가 연속해서 나올 확률 등을 반영
  - 단점
    - 학습 데이터가 많아야 함: 데이터가 부족하면 성능이 낮아질 수 있음
    - 딥러닝보다 성능이 낮음: 최근에는 BiLSTM-CRF, BERT 기반 모델이 더 높은 성능 제공

- CRF와 다른 모델 비교

| 모델 | 특징 | 장점 | 단점 |
|------|------|------|------|
| HMM | 이전 한 개의 상태만 반영 | 학습이 간단하고 빠름 | 긴 문맥 반영 어려움 |
| CRF | 전체 문맥을 반영 | 높은 정확도, 문맥 활용 가능 | 학습 데이터가 많아야 함 |
| BiLSTM-CRF | 딥러닝 기반 시퀀스 모델 | 장기 의존성 반영 가능 | 계산량이 많음 |
| BERT 기반 모델 | 사전 학습된 언어 모델 | 최고 성능, 문맥 완벽 반영 | 연산량이 많고 학습이 어려움 |

- CRF는 HMM보다 더 넓은 문맥을 고려할 수 있으며,  
- 딥러닝 기반 모델(BiLSTM-CRF, BERT)에 비해 해석 가능성이 높고 데이터 요구량이 상대적으로 적은 장점이 있습니다.

## CRF + 딥러닝 (BiLSTM-CRF)
최근에는 BiLSTM(양방향 LSTM)과 CRF를 결합하여 품사 태깅을 수행하는 모델(BiLSTM-CRF)이 주로 사용

```python
import torch
import torch.nn as nn
from torchcrf import CRF

class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=64, hidden_dim=128):
        super(BiLSTM_CRF, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True, batch_first=True)
        self.fc = nn.Linear(hidden_dim, tagset_size)
        self.crf = CRF(tagset_size, batch_first=True)

    def forward(self, x):
        x = self.embedding(x)
        x, _ = self.lstm(x)
        x = self.fc(x)
        return x

model = BiLSTM_CRF(vocab_size=1000, tagset_size=10)
```

- LSTM이 문맥을 학습하고 CRF가 최적의 태깅 시퀀스를 결정
- HMM보다 높은 성능을 제공하며, BERT와 함께 사용하면 더욱 강력함
- 결론
  - CRF는 HMM보다 강력한 모델로, 문맥을 반영하여 품사 태깅 수행 가능
  - 최근에는 BiLSTM-CRF, BERT 등 딥러닝 기반 모델이 더 높은 성능을 보이며 활용됨
  - 학습 데이터가 많으면 CRF는 매우 강력한 시퀀스 태깅 모델이 될 수 있음


## Character-Level BiLSTM-CRF
- Character-Level BiLSTM-CRF는 개별 단어가 아니라 문자를 단위(Character-Level)로 처리하는 품사 태깅(POS Tagging) 및 형태소 분석 모델입니다
- 한국어와 같은 형태소 언어에서 효과적이며, OOV(Out-of-Vocabulary) 문제 해결에 강점
- Character-Level BiLSTM-CRF 개념
  - BiLSTM (Bidirectional LSTM)
    - 문맥(Context)을 고려하기 위해 양방향(Bidirectional) LSTM을 사용
    - LSTM이 문자의 시퀀스를 학습하여 단어 벡터(Vector)로 변환
  - CRF (Conditional Random Field)
    - LSTM이 생성한 품사 태깅 후보들을 기반으로 최적의 시퀀스를 선택
    - 문맥(Context) 간 관계를 고려하여 보다 정확한 태깅 수행
  - Character-Level 방식의 장점
    - OOV(Out-of-Vocabulary) 문제 해결: 새로운 단어가 등장해도 문자 단위로 학습하여 태깅 가능
    - 띄어쓰기 오류 보완: 한국어에서 자주 발생하는 띄어쓰기 오류를 완화
    - 소형 데이터셋에서도 효과적: 단어가 아닌 문자(Character)를 학습하므로 데이터 요구량이 낮음
- Character-Level BiLSTM-CRF 동작 방식
  - 입력 문장 → 문자 단위로 변환
    - 예) "학교에 간다" → ['학', '교', '에', '간', '다']
  - 임베딩(Embedding) 레이어 적용
    - 각 문자(Character)를 고유한 벡터(Vector)로 변환
  - BiLSTM을 활용하여 문맥을 학습
    - 예: "학교"라는 단어의 의미를 ['학', '교'] 문맥에서 학습
  - CRF가 최적의 품사 태깅 시퀀스를 결정
    - "학교에 간다" → ['학교(NNG)', '에(JKB)', '간다(VV+EF)']

## Character-Level BiLSTM-CRF 모델 구현 (PyTorch)

- 데이터 전처리

```python
import torch
import torch.nn as nn
from torchcrf import CRF

# 문자(Char)와 품사 태그(POS) 인덱스 사전
char2idx = {'학': 0, '교': 1, '에': 2, '간': 3, '다': 4}
tag2idx = {'NNG': 0, 'JKB': 1, 'VV': 2, 'EF': 3}

# 입력 데이터 (예제: "학교에 간다")
X_train = torch.tensor([[char2idx['학'], char2idx['교'], char2idx['에'], char2idx['간'], char2idx['다']]], dtype=torch.long)
y_train = torch.tensor([[tag2idx['NNG'], tag2idx['JKB'], tag2idx['VV'], tag2idx['EF'], tag2idx['EF']]], dtype=torch.long)
```

- Character-Level BiLSTM-CRF 모델 구현
```pyhton
class CharBiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=64, hidden_dim=128):
        super(CharBiLSTM_CRF, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # 문자 임베딩
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True, batch_first=True)
        self.fc = nn.Linear(hidden_dim, tagset_size)  # Fully Connected Layer
        self.crf = CRF(tagset_size, batch_first=True)  # CRF Layer 추가

    def forward(self, x):
        x = self.embedding(x)  # 문자 벡터 변환
        x, _ = self.lstm(x)  # BiLSTM 적용
        emissions = self.fc(x)  # 태그 점수 계산
        return emissions

# 모델 초기화
model = CharBiLSTM_CRF(vocab_size=len(char2idx), tagset_size=len(tag2idx))
```

- 학습 과정

```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# 학습 루프
for epoch in range(10):
    optimizer.zero_grad()
    emissions = model(X_train)
    loss = -model.crf(emissions, y_train)  # CRF Loss 계산
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")
```

- 예측 수행

```python
with torch.no_grad():
    emissions = model(X_train)
    predicted_tags = model.crf.decode(emissions)  # CRF를 통해 최적 태깅 시퀀스 예측
    print("예측된 품사 태깅:", predicted_tags)

# > 예측된 품사 태깅: [[0, 1, 2, 3, 3]]  # ('학', NNG), ('교', NNG), ('에', JKB), ('간', VV), ('다', EF)
```

- Character-Level BiLSTM-CRF의 장점과 단점
  - 장점
    - OOV 문제 해결 → 새로운 단어가 등장해도 문자(Character) 단위로 처리 가능
    - 띄어쓰기 오류 보완 → 한국어에서 자주 발생하는 띄어쓰기 문제 완화
    - 소형 데이터셋에서도 효과적 → 문자 기반 학습이므로 데이터 요구량이 적음
    - BiLSTM의 문맥 학습 + CRF의 최적 시퀀스 예측으로 높은 성능 달성
  - 단점
    - 연산량 증가 → 문자가 많아질수록 학습 및 추론 속도가 느려짐
    - 문장 길이가 길어지면 성능 저하 가능
    - 형태소 분석기보다 정확도가 낮을 수 있음 (사전 기반 분석기와 결합 필요)

# Character-Level BiLSTM-CRF vs. 다른 모델 비교

| 모델 | 특징 | 장점 | 단점 |
|------|------|------|------|
| HMM | 이전 품사만 고려하는 확률 모델 | 학습이 간단 | 긴 문맥 반영 어려움 |
| CRF | 전체 문맥을 반영 | 문맥 기반 태깅 가능 | 학습 데이터 요구량 높음 |
| BiLSTM-CRF | LSTM이 문맥 학습 후 CRF가 최적 태깅 | 높은 정확도 | 학습 속도 느림 |
| Character-Level BiLSTM-CRF | 문자 단위 학습 | OOV 문제 해결 가능 | 연산량 증가 |

- Character-Level BiLSTM-CRF는 단어의 문자 단위 특징까지 학습하여  
- 신조어(OOV, Out-of-Vocabulary) 문제를 해결하는 데 효과적이지만,  
- 추가적인 연산량이 필요하여 학습과 추론 속도가 느려질 수 있습니다.
- Character-Level BiLSTM-CRF는 딥러닝 기반 모델 중에서도 OOV 문제 해결 능력이 뛰어나며, 문맥을 반영한 태깅을 수행하는 강력한 NLP 모델

- Character-Level BiLSTM-CRF의 실제 활용 사례
  - 형태소 분석(Morphological Analysis) → 한국어 단어를 의미 단위로 분석
  - 품사 태깅(POS Tagging) → BiLSTM을 활용해 단어의 품사 예측
  - 개체명 인식(NER, Named Entity Recognition) → 인물, 장소, 조직명 등 태깅
  - 철자 교정(Spell Correction) → 띄어쓰기 오류 및 오타 수정

- 결론
  - Character-Level BiLSTM-CRF는 문자 단위(Character-Level)로 BiLSTM이 문맥을 학습하고, CRF가 최적의 품사 태깅을 수행하는 모델
  - OOV 문제 해결에 강점을 가지며, 띄어쓰기 오류에도 강한 성능을 보임
  - 한국어, 일본어, 중국어와 같은 형태소 언어에서 효과적이며, NLP 태스크(품사 태깅, 개체명 인식 등)에 폭넓게 활용 가능
  - 최근에는 Transformer 기반 모델(BERT + CRF)이 더 높은 성능을 보이며, Character-Level BiLSTM-CRF는 OOV 해결을 위해 하이브리드 방식으로 활용


## 개체명 인식 (NER: Named Entity Recognition)
- 개체명 인식(Named Entity Recognition, NER)은 문장에서 특정 개체(이름, 장소, 조직, 날짜 등)를 식별하는 자연어 처리(NLP) 기술
- 검색, 챗봇, 문서 요약, 금융 데이터 분석, 감성 분석 등 다양한 응용 분야에서 활용
- 개체명 인식(NER)의 개념
  - NER이란?
    - 문장에서 특정 개체(Entity)를 인식하고 분류하는 과정
    - 예: "스티브 잡스는 애플을 공동 창립했다."
```
[('스티브 잡스', PERSON), ('애플', ORGANIZATION)]
```

- NER의 주요 목표
  - 개체명(Entity) 추출: 문장에서 특정 개체를 찾아냄
  - 개체명 분류(Entity Classification): 개체명을 적절한 범주(Person, Location, Organization 등)로 태깅
- NER 주요 응용 분야
  - 검색 엔진(Search Engine) → 사용자 질의어에서 개체명을 추출하여 검색 성능 향상
  - 챗봇(Chatbot) → 사용자 입력에서 특정 인물, 장소 등을 인식하여 적절한 응답 제공
  - 금융 및 뉴스 분석 → 주식 시장 및 경제 뉴스에서 회사명, 인물, 사건 탐지
- 의료 데이터 처리 → 환자 기록에서 약물, 질병명, 병원명 등을 추출
- 개체명(Named Entities) 종류

# 개체 유형 (Named Entity Types)

| 개체 유형 | 설명 | 예시 |
|----------|------|------|
| PER (Person, 인물) | 사람 이름 | 스티브 잡스, 정우성 |
| LOC (Location, 장소) | 도시, 국가, 지역 | 서울, 미국, 제주도 |
| ORG (Organization, 조직) | 기업, 단체, 기관 | 애플, 삼성전자, UN |
| DATE (날짜, 시간) | 날짜, 시간 정보 | 2023년 5월 1일, 오전 10시 |
| MONEY (금액) | 화폐 단위가 포함된 금액 | 100만원, 10달러 |
| TIME (시간) | 특정한 시간 표현 | 오전 3시, 오후 2시 |
| GPE (Geopolitical Entity, 지리적 정치 단위) | 국가, 도시, 지역 | 대한민국, 뉴욕, 캘리포니아 |

- NER을 위한 기법
  - 전통적인 접근 방식
    - 사전(Dictionary) 기반 방식
      - 미리 정의된 개체명 리스트를 사용하여 일치하는 항목을 탐색
      - 단점: 신조어나 새로운 개체명을 탐지하기 어려움
  - 규칙 기반(Rule-based) 방식
    - 특정 패턴(정규 표현식, 접미사 분석 등)을 기반으로 개체명을 추출
    - 예: "[0-9]+년 [0-9]+월 [0-9]+일" → 날짜 인식
    - 단점: 복잡한 문장에서는 한계 발생
  - 통계적 모델(Statistical Model)
    - HMM (Hidden Markov Model), CRF (Conditional Random Field) 사용
    - 문맥을 반영하여 개체명을 탐지 가능
  - 딥러닝 기반 접근 방식
    - BiLSTM-CRF
    - BiLSTM이 문맥(Context)을 학습하고, CRF가 최적의 개체명 시퀀스를 예측
    - 한국어와 같은 교착어 처리에 효과적
  - Transformer 기반 모델 (BERT + CRF)
    - BERT가 단어 임베딩을 학습하고 CRF가 개체명을 탐색
    - 최신 연구에서 가장 높은 성능을 보이며, 한국어 NER에도 효과적

- BiLSTM-CRF를 활용한 NER 구현 (Python)

```python
# > 데이터전처리
import torch
import torch.nn as nn
from torchcrf import CRF

# 개체명 태그
tag2idx = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6}

# 샘플 문장 데이터 ("스티브 잡스는 애플에서 일했다.")
sentence = ["스티브", "잡스", "는", "애플", "에서", "일했다"]
labels = ["B-PER", "I-PER", "O", "B-ORG", "O", "O"]

# 인덱스 변환
X_train = torch.tensor([[0, 1, 2, 3, 4, 5]], dtype=torch.long)
y_train = torch.tensor([[tag2idx[tag] for tag in labels]], dtype=torch.long)

# > BiLSTM-CRF 모델 구현
class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size, embedding_dim=64, hidden_dim=128):
        super(BiLSTM_CRF, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=1, bidirectional=True, batch_first=True)
        self.fc = nn.Linear(hidden_dim, tagset_size)
        self.crf = CRF(tagset_size, batch_first=True)

    def forward(self, x):
        x = self.embedding(x)  # 임베딩
        x, _ = self.lstm(x)  # BiLSTM
        emissions = self.fc(x)  # 태그 예측 점수
        return emissions

# 모델 초기화
model = BiLSTM_CRF(vocab_size=1000, tagset_size=len(tag2idx))

# > 학습과정
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

for epoch in range(10):
    optimizer.zero_grad()
    emissions = model(X_train)
    loss = -model.crf(emissions, y_train)  # CRF Loss
    loss.backward()
    optimizer.step()
    print(f"Epoch {epoch+1}, Loss: {loss.item()}")

# > 예측 수행
with torch.no_grad():
    emissions = model(X_train)
    predicted_tags = model.crf.decode(emissions)
    print("예측된 개체명 태깅:", predicted_tags)

# > 출력 예시
# 예측된 개체명 태깅: [[1, 2, 0, 3, 0, 0]]  # ('스티브', B-PER), ('잡스', I-PER), ('애플', B-ORG)

```

- 딥러닝 기반 NER 모델 비교

| 모델 | 특징 | 장점 | 단점 |
|------|------|------|------|
| HMM | 이전 개체명만 고려 | 간단한 구현 | 긴 문맥 반영 어려움 |
| CRF | 문맥 고려 가능 | 높은 정확도 | 학습 데이터 요구 |
| BiLSTM-CRF | 문맥 학습 후 CRF 적용 | 한국어 처리 효과적 | 계산량 많음 |
| BERT + CRF | Transformer 기반 | 최고 성능 | 연산량 증가 |

- 최신 NER에서는 `BERT + CRF`가 가장 높은 성능을 보이며, `BiLSTM-CRF`는 데이터가 적을 때 유용함.  
- BERT는 사전 학습된 언어 모델을 활용하여 문맥을 깊이 이해하고,  
- CRF는 출력 시퀀스 간의 의존성을 최적화하여 더욱 정교한 개체명 인식이 가능합니다.

- 개체명 인식의 실제 활용 사례
  - 금융 뉴스 분석 → 기업명, 주식 정보 추출
  - 의료 데이터 분석 → 환자 기록에서 질병명, 약물명 탐색
  - 법률 문서 요약 → 사건 관련 인물, 조직, 날짜 추출
  - 검색 엔진 및 챗봇 → 검색 정확도 및 대화 흐름 개선
- 결론
  - NER은 문장에서 개체명을 자동으로 인식하고 분류하는 NLP 기술
  - BiLSTM-CRF, BERT-CRF 모델이 높은 성능을 보이며, 한국어에서도 효과적
  - OOV 문제 해결 및 긴 문맥 반영이 가능하여 다양한 산업에서 활용
  - 즉, 개체명 인식(NER)은 검색, 금융, 법률, 의료 분야에서 필수적인 기술로 자리 잡고 있음

## 태깅 시스템
- 태깅 시스템(Tagging System)은 데이터를 특정 범주(Category) 또는 속성(Attribute)으로 분류하여 관리하는 기법
- 태깅 시스템의 개념
  - 태그(Tag)란?
  - 특정 데이터에 의미 있는 키워드(Label)를 부착하는 방식
  - 예: 블로그 포스트에 "AI", "딥러닝", "자연어 처리" 태그 추가
  - 문장에서 "삼성전자" → ORG(조직), "2024년" → DATE(날짜) 태깅
- 태깅 시스템(Tagging System)이란?
  - 데이터에 태그를 자동 또는 수동으로 추가하여 검색 및 분류를 효율적으로 수행하는 시스템
  - 주요 목적:
    - 데이터 검색 속도 향상
    - 데이터 필터링 및 추천 시스템 개선
    - 텍스트 및 이미지 분류 자동화
- 태깅 시스템의 주요 유형
  - 수동 태깅(Manual Tagging) → 사용자가 직접 태그 입력
  - 자동 태깅(Auto Tagging) → NLP, 머신러닝, 딥러닝 기반 태그 자동 생성
  - 하이브리드 태깅(Hybrid Tagging) → 수동 + 자동 태깅 결합
- 태깅 시스템의 주요 응용 분야

| 분야 | 활용 사례 |
|------|------|
| 문서 관리 (Document Management) | 문서에 키워드 태그 추가하여 검색 최적화 |
| 검색 엔진 (Search Engine) | 태깅된 데이터를 기반으로 관련 문서 검색 |
| 소셜 미디어 (Social Media) | 해시태그(#)를 활용한 콘텐츠 분류 및 추천 |
| 전자상거래 (E-commerce) | 상품에 카테고리 태깅하여 필터링 및 추천 시스템 개선 |
| 자연어 처리 (NLP) | 품사 태깅(POS Tagging), 개체명 인식(NER) |
| 이미지 분석 (Image Processing) | AI 기반 이미지 자동 태깅 |

- 태깅 시스템의 종류
  - 품사 태깅(POS Tagging)
  - 단어에 품사(Part-of-Speech, POS) 정보를 부착하는 태깅 기법
```
나는 학교에 간다
['나는/NP', '학교에/NNG', '간다/VV']
```
  - 대표적인 모델
    - HMM (Hidden Markov Model)
    - CRF (Conditional Random Field)
    - BiLSTM-CRF
    - BERT 기반 태깅
- 개체명 인식(NER: Named Entity Recognition)
- 감성 분석 태깅(Sentiment Tagging)
  - 문장에서 감정을 분석하여 긍정/부정 태그 부착
  - 대표적인 모델
    - LSTM + Attention
    - BERT + Sentiment Classification
  - 예시
```
이 영화 정말 좋았어! → 긍정(Positive)
이 서비스는 최악이야. → 부정(Negative)
```

- 텍스트 분류 태깅(Text Classification Tagging)
  - 문서를 특정 카테고리로 태깅
  - 대표적인 모델
    - CNN + RNN
    - BERT Text Classification
  - 예시
```
제목: "삼성전자, AI 반도체 출시"
태그: ['IT', '반도체', 'AI']
```

- 태깅 시스템 구현 예제 (간단한 자동 태깅 시스템)

```python
from transformers import pipeline

# BERT 기반 NER 모델 로드
ner_model = pipeline("ner", model="dbmdz/bert-large-cased-finetuned-conll03-english")

# 예제 문장
sentence = "Elon Musk is the CEO of Tesla."

# NER 태깅 수행
tags = ner_model(sentence)

# 결과 출력
for tag in tags:
    print(f"단어: {tag['word']}, 개체명: {tag['entity']}")

# 출력 예시
#> 단어: Elon, 개체명: B-PER
#> 단어: Musk, 개체명: I-PER
#> 단어: Tesla, 개체명: B-ORG
```

- 한국어 품사 태깅 (KoNLPy 활용)

```python
from konlpy.tag import Okt

okt = Okt()
text = "나는 학교에 간다."

# 품사 태깅
print(okt.pos(text))

# 출력예시
# > [('나', 'Noun'), ('는', 'Josa'), ('학교', 'Noun'), ('에', 'Josa'), ('간다', 'Verb')]
```

- 태깅 시스템의 장점과 단점
  - 장점
    - 데이터 검색 및 관리 효율성 증가 → 태그를 기반으로 데이터 정리 가능
    - 추천 시스템 개선 → 태그를 활용한 개인화 추천 가능
    - 자연어 이해 향상 → 문맥을 고려한 자동 태깅 가능
    - AI 기반 태깅 자동화 가능 → ML/DL 모델을 통해 자동화 가능
  - 단점
    - 잘못된 태깅 가능성 → 수동 태깅 시 오류 가능, 자동 태깅은 오탐 위험
    - 태그 표준화 필요 → 동일한 개념이라도 태그명이 다르면 검색 효율 저하
    - 대규모 데이터 처리 시 비용 발생 → 태깅된 데이터를 실시간 처리하기 위한 인프라 필요

- 태깅 시스템과 추천 시스템의 연계
  - 태깅 데이터를 활용하여 추천 시스템 개선
  - 예시: 영화 추천
    - 사용자가 "액션", "마블", "SF" 태그가 붙은 영화를 자주 시청하면 → 유사한 태그의 영화를 추천
    - 사용자의 리뷰에서 감성 분석을 수행하여 "긍정적 리뷰가 많은 영화 추천

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# 예제 태그 데이터
tags = ["액션 SF", "드라마 로맨스", "코미디 가족", "스릴러 공포"]

# TF-IDF 벡터화
vectorizer = TfidfVectorizer()
tag_vectors = vectorizer.fit_transform(tags)

print(vectorizer.get_feature_names_out())

# 출력
# > ['SF' '공포' '가족' '드라마' '로맨스' '스릴러' '액션' '코미디']

```

## 정보추출(Information Extraction)

- 정보추출(Information Extraction, IE)은 비정형 텍스트에서 의미 있는 정보를 자동으로 식별하고 구조화하는 과정
- 자연어 처리(NLP)에서 개체명 인식(NER), 관계 추출(RE), 이벤트 추출(EE) 등의 방법이 사용

- 정보추출(IE)의 구조
  - 정보추출 시스템은 일반적으로 다음과 같은 구조(Workflow)를 가집니다.

    - 입력 데이터 (Raw Text)
      - 뉴스, 논문, 리뷰, 소셜미디어 등에서 텍스트 수집
      - 예: "스티브 잡스는 애플을 공동 창립했다."
    - 개체명 인식 (NER, Named Entity Recognition)
      - 문장에서 사람, 장소, 조직, 날짜, 수량 등의 개체(Entity)를 인식
      - 예: ["스티브 잡스" (PER), "애플" (ORG)]
    - 관계 추출 (RE, Relation Extraction)
      - 인식된 개체들 간의 관계(Relation)를 인식
      - 예: ["스티브 잡스" → 공동 창립(Relation) → "애플"]
    - 이벤트 추출 (Event Extraction)
      - 특정 이벤트(Event)와 관련된 정보를 추출
      - 예: ["창립(설립)", 주체: "스티브 잡스", 대상: "애플", 날짜: 1976년]
    - 지식 그래프 구축 (Knowledge Graph)
      - 개체와 관계를 연결하여 지식 그래프(KG) 생성
        - (스티브 잡스) — [설립] → (애플)
- 정보추출(IE) 접근 방법
  - 전통적인 접근법
    - 규칙 기반(Rule-based) 방법
    - 기계 학습 기반(Statistical) 방법
  - 딥러닝 기반 접근법
    -  BiLSTM-CRF (Bidirectional LSTM + CRF)
    - Transformer 기반 (BERT, GPT)
- 텍스트 분류 (Text Classification)
  - 텍스트 분류는 주어진 문장을 특정 카테고리로 자동 분류하는 NLP 기술
  - 예를 들어 뉴스 기사 분류, 감성 분석, 스팸 필터링, 법률 문서 자동 분류 등에 사용
  - 주요 개념
    - 텍스트 분류란?
      - 문서를 하나 이상의 카테고리(Label)로 분류하는 작업
  - 주요 기법
    - 전통적인 기법
      - TF-IDF + 머신러닝 모델
      - TF-IDF (Term Frequency - Inverse Document Frequency)
      - 자주 등장하는 단어(TF)와 희귀 단어(IDF)를 반영하여 벡터화
        - 머신러닝 분류기 사용:
          - SVM (Support Vector Machine)
          - Naive Bayes
          - Random Forest
          - 장점: 연산 속도 빠름, 작은 데이터에서도 효과적
          - 단점: 문맥을 고려하기 어려움
        - Word2Vec + 분류 모델
          - Word2Vec을 사용하여 단어를 벡터로 변환 후 분류
          - 장점: 단어 의미를 반영 가능
          - 단점: 긴 문장에서는 성능 저하
    - 딥러닝 기반 기법
      - CNN for Text Classification
        - 문장에서 n-gram 특징을 CNN 필터로 학습하여 분류
        - 장점: 짧은 문장에서 효과적
        - 단점: 문맥 정보 부족
      - LSTM / BiLSTM for Text Classification
        - 문맥을 반영한 순차 데이터 학습 가능
        - 장점: 긴 문장에서 성능 우수
        - 단점: 학습 속도 느림
    - Transformer 기반 (BERT, GPT)
      - 사전 학습된 BERT 모델을 활용하여 문맥 기반 분류 수행
      - 장점: 문맥을 가장 깊이 이해하여 최고 성능
      - 단점: 연산량 많고 대량 데이터 필요
- 간단한 TF-IDF + Naive Bayes 텍스트 분류 예제

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import make_pipeline

# 샘플 데이터
texts = ["이 제품 정말 좋아요!", "최악의 경험이었어요.", "삼성전자가 AI 반도체를 출시했다."]
labels = ["Positive", "Negative", "IT"]

# 모델 생성
model = make_pipeline(TfidfVectorizer(), MultinomialNB())
model.fit(texts, labels)

# 예측
print(model.predict(["애플이 새로운 아이폰을 발표했다."]))


# 결과 : ['IT']   
```

- 텍스트 분류 모델 비교

| 모델 | 특징 | 장점 | 단점 |
|------|------|------|------|
| TF-IDF + Naive Bayes | 단순한 문서 분류 | 빠름, 적은 데이터로 학습 가능 | 문맥 고려 불가능 |
| CNN for Text | n-gram 특징 학습 | 짧은 문장에서 효과적 | 문맥 이해 부족 |
| LSTM / BiLSTM | 순차 데이터 학습 | 긴 문장 분류에 효과적 | 학습 속도 느림 |
| BERT 기반 분류 | 문맥을 반영한 최고 성능 | 높은 정확도 | 연산량 많음 |

- 결론
  - 정보추출(IE)는 문장에서 개체명(NER), 관계 추출(RE), 이벤트 추출(EE)을 수행
  - 텍스트 분류는 머신러닝부터 BERT 기반 딥러닝까지 다양한 기법 사용
  - 최신 딥러닝 모델(예: BERT + Transformer)이 가장 높은 성능을 제공하며, 실제 산업에서도 활용 중 🚀


## 문서 요약 (Text Summarization)
- 문서 요약은 긴 텍스트에서 핵심 정보를 자동으로 요약하는 NLP 기술
- 크게 추출 요약(Extractive Summarization)과 생성 요약(Abstractive Summarization)으로 나뉨
  - 추출 요약 (Extractive Summarization)
    - 문장에서 중요한 문장을 직접 선택하여 요약
    - 예: TextRank 알고리즘
    - 장점: 원문을 유지하여 정보 손실이 적음
    - 단점: 문장을 자연스럽게 구성하기 어려움
  - 생성 요약 (Abstractive Summarization)
    - 원문의 의미를 이해하고 새로운 문장으로 요약
    - 예: Transformer 기반 모델 (BART, T5)
    - 장점: 보다 자연스러운 문장 생성 가능
    - 단점: 정보 왜곡 가능
- Hugging Face를 활용한 BART 요약

```python
from transformers import pipeline

# BART 요약 모델
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")

# 입력 텍스트
text = """자연어 처리는 컴퓨터가 인간의 언어를 이해하고 처리하는 기술로, 기계 번역, 감성 분석, 문서 요약 등의 다양한 응용 분야에서 사용된다."""

# 요약 수행
summary = summarizer(text, max_length=30, min_length=10, do_sample=False)
print(summary[0]['summary_text'])

# 출력 : "자연어 처리는 기계 번역, 감성 분석, 문서 요약 등에 활용되는 기술이다."
```

- 문법 교정 데이터셋 (Grammar Correction Datasets)
  - 문법 교정(Grammar Correction)은 문장에서 문법 오류를 감지하고 수정하는 NLP 기술
- 한국어 문법 교정 데이터셋

| 데이터셋 | 제공 기관 | 데이터 규모 | 특징 |
|----------|----------|------------|------|
| AI-Hub 한국어 문법 오류 데이터셋 | NIA | 50만 문장+ | 맞춤법, 띄어쓰기 오류 포함 |
| ETRI 한국어 문법 오류 코퍼스 | ETRI | 약 10만 문장 | 문장 오류 수정 데이터 |
| Hugging Face 한국어 문법 데이터 | Hugging Face | 5만 문장 | 문법 오류 수정 샘플 제공 |

- 예시 코드

```python
from transformers import pipeline

# 문법 교정 모델 (영어 예제)
grammar_corrector = pipeline("text2text-generation", model="prithivida/grammar-error-correcter")

# 문장 수정
text = "She go to school every day."
corrected_text = grammar_corrector(text)[0]['generated_text']
print(corrected_text)

# 출력: She goes to school every day.
```

## 통계 기반 NLP (Statistical NLP)
- 통계 기반 NLP는 기계 학습을 활용하여 자연어 데이터를 분석하는 방식
- 주요 기법
  - n-그램 (n-gram) → 문맥을 고려한 확률 모델
  - Hidden Markov Model (HMM) → 순차적 확률 모델
  - CRF (Conditional Random Field) → 개체명 인식(NER) 등에 활용
- n-그램 모델 예제

```python
from nltk import ngrams

text = "자연어 처리는 중요하다."
bigrams = list(ngrams(text.split(), 2))
print(bigrams)

# 출력: [('자연어', '처리는'), ('처리는', '중요하다.')]
```

## 뉴럴심볼릭 기반 자연어처리 (Neural-Symbolic NLP)
- 뉴럴심볼릭 NLP(Neural-Symbolic NLP)는 기호 기반(Symbolic AI)과 딥러닝(Neural Networks)을 결합한 방식
- 뉴럴심볼릭 NLP 개념
  - 딥러닝(Neural Networks) → 데이터에서 패턴 학습
  - 기호 논리(Semantic Logic) → 언어 규칙, 지식 그래프 활용
  - 하이브리드 모델 → 규칙 + 뉴럴 네트워크 결합
- 뉴럴심볼릭 NLP 예제
  - BERT + Knowledge Graph (KG)
  - GPT + 논리 추론(Logical Inference)
- 예제

```python
from transformers import pipeline

# BERT 기반 개체명 인식 (NER)
ner_pipeline = pipeline("ner", model="dbmdz/bert-large-cased-finetuned-conll03-english")

# 입력 텍스트
text = "Elon Musk founded SpaceX."

# 개체명 인식
ner_results = ner_pipeline(text)
print(ner_results)

# 출력: [{'word': 'Elon', 'entity': 'B-PER'}, {'word': 'Musk', 'entity': 'I-PER'}, {'word': 'SpaceX', 'entity': 'B-ORG'}]
```

위 목록은 딥러닝 기반 자연어 처리(NLP)의 핵심 개념을 정리한 내용입니다. 각 항목을 간략하게 요약해 드리겠습니다.

---

# Sequence-to-Sequence 이해하기
- 인코더(Encoder): 입력 시퀀스를 받아 고정된 길이의 벡터로 변환
- 디코더(Decoder): 인코더에서 생성한 벡터를 기반으로 출력 시퀀스를 생성  
  💡 사용 예시: 기계 번역(예: 영어 → 한국어), 대화 모델, 텍스트 요약

---

# Recurrent Neural Networks (RNNs)
- 순차 데이터(문장, 음성 등)를 처리하는 인공신경망  
- 단어별로 시간 순서에 따라 처리하는 방식 → 순환 구조(Recurrent)

## LSTM & GRU 이해하기
### Gradient Vanishing / Exploding
- RNN 학습 시 기울기가 너무 작아지거나 커지는 문제  
- Gradient Vanishing: 초반 정보가 후반부에서 사라지는 현상  
- Gradient Exploding: 기울기가 너무 커져서 학습이 불안정해짐  

### Long Short-Term Memory (LSTM)
- 장기 기억(Long-term dependency) 문제 해결  
- Forget Gate, Input Gate, Output Gate 구조로 중요한 정보만 저장  

### Gated Recurrent Unit (GRU)
- LSTM보다 더 단순한 구조, 계산량 감소  
- Update Gate, Reset Gate로 정보 유지 및 삭제 조절  

### RNN vs LSTM / GRU
| 비교 | RNN | LSTM | GRU |
|------|------|------|------|
| 구조 | 간단 | 복잡 | 중간 |
| 장기 의존성 | 취약 | 강함 | 강함 |
| 연산량 | 적음 | 많음 | LSTM보다 적음 |
| 학습 속도 | 빠름 | 느림 | 상대적으로 빠름 |

---

# Attention이란?
- RNN이 과거 정보를 잊는 문제 해결  
- 입력 시퀀스의 모든 정보를 가중치를 부여해 학습  
- 번역, 문서 요약, 개체명 인식 등에 사용

## Attention의 핵심 개념
### Long-term Dependency
- 긴 문장일수록 초반 정보가 사라지는 문제 해결  
- Attention은 모든 단어를 동시에 참고하여 해결  

### Attention의 효과
- 문맥 정보를 효과적으로 반영  
- 번역 모델에서 핵심 단어 선택 가능

### Attention in Computer Vision
- 이미지 특징을 추출하여 어디에 집중할지 결정  
- 이미지 캡셔닝, 객체 검출 등에 활용

### Attention 이해하기
- Query, Key, Value: 각 단어가 다른 단어와의 관계를 학습하는 방법  
- Dot-product Attention: 내적(dot-product)으로 가중치를 계산  
- Bahdanau Attention: RNN에서 많이 쓰이는 attention 기법  
- Attention Map: 입력 단어별 가중치를 시각화한 행렬  
- RNN with Attention: 기존 RNN 모델에 Attention 추가  

---

# Transformer
RNN 없이 병렬로 학습하는 구조로, 번역, 문서 요약, 대화 모델 등에서 필수적인 아키텍처

## 구성요소
### Transformer Encoder
1. Input Embeddings: 단어를 벡터로 변환
2. Positional Encoding: 단어 순서 정보 추가
3. Self-Attention: 단어 간 관계를 학습
4. Multi-Head Attention: 다양한 관점에서 Self-Attention 수행
5. Addition & Layer Normalization: 학습 안정화
6. FFNN (Feed-Forward Networks): 비선형 변환 적용
7. Causal Attention: 이전 단어만 참고하도록 제한
8. Encoder-Decoder Attention: 인코더에서 받은 정보를 디코더에서 사용
9. Linear & Softmax: 최종 단어 확률 출력

---

# BERT(Bidirectional Encoder Representations from Transformers)
- 양방향 학습을 통해 문맥을 깊이 이해하는 모델  
- MLM (Masked Language Modeling): 문장의 일부 단어를 가려놓고 예측  
- NSP (Next Sentence Prediction): 두 문장이 연속된 문장인지 예측  
- 응용 분야: 질의응답, 문서 요약, 감성 분석, 검색엔진 등

---

# GPT-2(Generative Pretrained Transformer 2)
- 언어 생성(Language Generation)에 특화된 모델  
- 대량의 텍스트를 학습하여 자연스러운 문장 생성 가능  
- 차이점: BERT는 입력 문맥을 이해하는 모델, GPT는 문장을 생성하는 모델

---

# BART(Bidirectional Auto Regressive Transformers)
- BERT + GPT 개념 결합 (BERT처럼 인코딩하고, GPT처럼 생성)
- BART Pretraining 기법
  - Token Masking: 일부 단어를 마스킹하여 예측  
  - Token Deletion: 일부 단어를 삭제하고 복원  
  - Text Infilling: 문장 일부를 비우고 예측  
  - Sentence Permutation: 문장 순서를 섞어 예측  
- 응용: 문서 요약, 질의응답, 기계 번역

---

# 딥러닝 기반 형태소 분석 & 품사 태깅
- CNN, RNN 기반으로 형태소 분석 모델을 학습하여 문장을 해석  
- 품사 태깅(POS tagging)과 개체명 인식(NER)에 활용  

## CNN 방식의 형태소 분석과 품사 태깅
- CNN을 활용하여 글자(Character)-단위 패턴을 학습  
- 형태소 단위 예측을 수행  

## Character-Level Bidirectional LSTM-CRF
- 단어가 아닌 문자(Char)-단위로 분석  
- LSTM + CRF(Conditional Random Fields) 조합  
- 개체명 인식(NER), 품사 태깅, 문장 분류 등에 사용

---

# 딥러닝 기반 의미역 분석
- 문장에서 각 단어가 수행하는 역할(주어, 목적어, 동사 등)을 파악  
- 기계 번역, 질의응답, 챗봇 등에 필수  

---

# 딥러닝 기반 개체명 인식 (NER)
- 사람, 장소, 날짜, 제품명 등을 자동으로 인식하는 기술  
- 챗봇, 문서 분석, 검색 엔진 등에 활용  

---

# 딥러닝 기반 질의응답 (QA)
- 사용자의 질문을 이해하고, 적절한 답변을 생성  
- 대표 모델: BERT-QA, DrQA, T5  
- 검색 기반 QA와 생성 기반 QA 방식이 존재  

---

# 딥러닝 기반 문서 요약
- 주요 기법
  - BART Summarization: BART를 활용한 요약  
  - KoBART: 한국어 요약을 위해 BART를 최적화  
  - BERTSum: BERT 기반 추출 요약  
  - STEP (Sequence-To-Sequence Pre-training)  
  - PEGASUS: 문서 요약 최적화 모델  

---

# 딥러닝 기반 대화 모델
- Dialogue System: 챗봇과 같은 대화 시스템 구축  
- Task-Oriented Dialogue (TOD): 특정 목적(예: 호텔 예약)에 맞춘 대화  
- TOD-BERT: Task-Oriented Dialogue 최적화 모델  
- RNN 기반 문장 생성: Seq2Seq 방식  
- Transformer 기반 문장 생성: GPT, DialoGPT 활용  

---

## 결론
- 딥러닝 기반 NLP는 Transformer를 중심으로 발전  
- BERT, GPT, BART 등 다양한 모델이 각기 다른 목적에 맞게 최적화됨  
- Attention 메커니즘이 NLP뿐만 아니라 컴퓨터 비전에도 활용됨  
- 문서 요약, 챗봇, 질의응답 등 다양한 분야에서 실용적으로 적용 가능

# 25.03.10(월)

# Seq2Seq (Sequence to Sequence)

## 인코더 (Encoder)
- 입력 시퀀스를 받아 고정된 크기의 컨텍스트 벡터(context vector)로 변환하는 역할을 수행
- 주로 RNN (Recurrent Neural Network), LSTM (Long Short-Term Memory), GRU (Gated Recurrent Unit) 등의 구조 사용
- 각 입력 단어를 벡터로 변환하고 시퀀스를 따라 순차적으로 처리하여 최종적인 하나의 벡터로 요약
- 마지막 히든 스테이트(hidden state)를 디코더에 전달하여 문맥(Context)를 유지

## 디코더 (Decoder)
- 인코더에서 전달된 컨텍스트 벡터를 기반으로 출력 시퀀스를 생성하는 역할 수행
- 처음에는 인코더의 최종 히든 스테이트를 받아 첫 번째 단어를 생성한 후, 생성된 단어를 다시 입력하여 다음 단어를 예측하는 방식으로 동작
- 일반적으로 Teacher Forcing 기법을 사용하여 학습 (정답 시퀀스를 일부 제공)

## Seq2Seq 구조
- 입력 시퀀스 → 인코더 → 컨텍스트 벡터 → 디코더 → 출력 시퀀스
- 단순한 RNN 기반의 Seq2Seq 모델은 긴 문장을 처리하는 데 어려움이 있음 → 해결책: Attention Mechanism

---

# RNN (Recurrent Neural Network)

## One-to-One, One-to-Many, Many-to-One, Many-to-Many
- One-to-One: 일반적인 신경망 (예: 이미지 분류)
- One-to-Many: 하나의 입력으로 여러 개의 출력 생성 (예: 이미지 캡셔닝)
- Many-to-One: 여러 개의 입력이 하나의 출력으로 변환 (예: 감정 분석)
- Many-to-Many: 여러 개의 입력이 여러 개의 출력으로 변환 (예: 기계 번역, 챗봇)

## RNN의 장단점
### 장점
- 순차 데이터 처리에 강점 (텍스트, 음성, 시계열 데이터 등)
- 과거 정보를 활용할 수 있음 (Hidden State 유지)

### 단점
- Vanishing Gradient 문제 (기울기가 소멸하여 장기 의존성 학습이 어려움)
- 긴 문장에서 기억력이 약함 → 해결책: LSTM, GRU

---

# LSTM (Long Short-Term Memory)

## LSTM의 구조
- Cell State를 추가하여 장기 의존성을 유지
- 3가지 게이트 사용
  1. Forget Gate: 불필요한 정보 삭제
  2. Input Gate: 새로운 정보 추가
  3. Output Gate: 최종 결과 계산

## LSTM의 장단점
### 장점
- 장기 의존성 문제 해결 (Vanishing Gradient 문제 완화)
- 긴 문장에서도 정보를 유지 가능

### 단점
- 계산량이 많음 (복잡한 구조)
- 학습 속도가 느림

---

# GRU (Gated Recurrent Unit)

## GRU의 구조
- LSTM과 유사하지만 Gate 수가 2개로 단순화됨
  1. Reset Gate: 과거 정보를 얼마나 반영할지 결정
  2. Update Gate: 새로운 정보와 과거 정보를 어떻게 결합할지 결정

## GRU의 장단점
### 장점
- LSTM보다 빠르고 경량
- 계산량이 적어 학습 속도가 빠름

### 단점
- LSTM보다 유연성이 낮음 (정보 삭제 방식이 단순함)
- 일부 문제에서는 LSTM보다 성능이 낮을 수 있음

---

# Attention

## Attention의 구조
- RNN 기반 Seq2Seq 모델에서 중요한 정보에 집중할 수 있도록 개선한 기법
- 입력 시퀀스의 각 단어가 출력 단어와 얼마나 관련 있는지 가중치(weight)로 계산
- 디코더가 모든 입력을 동시에 참고 가능 (컨텍스트 벡터의 한계를 극복)

## Attention의 장단점
### 장점
- 긴 문장에서도 정보 손실이 적음
- 병렬 연산 가능 (Transformer에서 효과적 활용)

### 단점
- 추가적인 계산 비용 발생
- 초반에는 최적의 가중치 학습이 어려울 수 있음

## RNN + Attention
- 기존 Seq2Seq 모델에서 컨텍스트 벡터 하나만 사용하던 방식에서 벗어나
- 입력 단어별로 가중치를 적용하여 보다 유연한 디코딩 가능

---

# Transformer

## Transformer의 구조
- RNN 없이도 시퀀스를 처리할 수 있는 모델
- Self-Attention Mechanism을 기반으로 동작
- Encoder-Decoder 구조 유지
- Multi-Head Attention, Feed-Forward Networks 사용

## Transformer의 장단점
### 장점
- 병렬 연산 가능 (학습 속도 빠름)
- 긴 문장에서도 정보 손실 없음

### 단점
- 학습에 많은 데이터와 연산 자원 필요

## Self-Attention이란?
- 시퀀스 내 단어들이 서로 어떻게 관련되는지를 학습하는 메커니즘
- 입력 문장의 각 단어가 다른 모든 단어들과 관계를 맺음

## Self-Attention 구조
1. Query, Key, Value 생성
2. Query와 Key의 유사도 계산 (Attention Score)
3. Score를 가중치로 활용하여 Value를 조합

## Multi-Head Attention이란?
- Self-Attention을 여러 번 수행하여 다양한 관점에서 정보를 학습

## Multi-Head Attention 구조
- 여러 개의 Attention Head가 서로 다른 부분을 학습하여 정보 손실 방지

## Query, Key, Value
- Query (Q): 현재 단어가 어떤 정보를 찾을지 결정
- Key (K): 모든 단어의 정보 저장
- Value (V): 출력할 정보 저장

## Query, Key, Value 연산하기
- $Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$

## Position-wise Feed-Forward Networks
- 각 Attention 레이어의 출력값을 변환하는 비선형 변환 층
- 모든 단어에 개별 적용되어 연산이 독립적임

## Positional Encoding
- Transformer는 RNN처럼 순서를 직접 학습하지 않음 → 위치 정보를 추가해야 함
- 사인(sin)과 코사인(cos) 함수를 활용하여 단어의 위치 정보를 인코딩
# 자연어 처리 평가지표 (Evaluation Metrics)

자연어 처리(NLP) 모델의 성능을 평가하기 위한 다양한 평가지표가 존재한다. 주어진 문제 유형에 따라 적절한 평가지표를 선택하는 것이 중요하다.

## 1. 분류(Classification) 평가 지표

- 정확도(Accuracy): 전체 데이터 중에서 올바르게 분류된 샘플의 비율.
  $$ \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} $$
  하지만 불균형한 데이터에서는 적절하지 않을 수 있다.

- 정밀도(Precision): 모델이 양성으로 예측한 것 중에서 실제 양성의 비율.
  $$ \text{Precision} = \frac{TP}{TP + FP} $$

- 재현율(Recall): 실제 양성 샘플 중에서 모델이 올바르게 양성으로 예측한 비율.
  $$ \text{Recall} = \frac{TP}{TP + FN} $$

- F1 Score: 정밀도와 재현율의 조화 평균.
  $$ F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall} $$

## 2. 회귀(Regression) 평가 지표

- Mean Squared Error (MSE): 예측값과 실제값의 차이를 제곱하여 평균한 값.
- Mean Absolute Error (MAE): 예측값과 실제값의 차이의 절대값 평균.
- R-squared (R^2 Score): 모델이 데이터를 얼마나 잘 설명하는지 평가하는 지표.

## 3. 언어 모델 평가 지표

- Perplexity (PPL): 언어 모델이 다음 단어를 얼마나 정확히 예측하는지 평가하는 지표. 낮을수록 성능이 좋다.
- BLEU (Bilingual Evaluation Understudy): 기계 번역 성능 평가 지표로, 생성된 문장이 실제 문장과 얼마나 유사한지를 측정.
- ROUGE (Recall-Oriented Understudy for Gisting Evaluation): 요약 및 생성된 텍스트가 참조 텍스트와 얼마나 유사한지 평가.

---

# 자연어 처리 Pipeline

자연어 처리의 기본적인 처리 과정은 다음과 같다.

1. 텍스트 수집(Data Collection): 원시 데이터를 수집한다.
2. 전처리(Preprocessing): 토큰화(Tokenization), 불용어 제거, 어간추출(Stemming) 등을 수행한다.
3. 특징 추출(Feature Extraction): 단어 임베딩 기법(Word2Vec, FastText, BERT)을 적용한다.
4. 모델 학습(Training Model): 딥러닝 모델(Transformer, RNN) 또는 전통적인 모델(SVM, Naive Bayes)을 학습한다.
5. 모델 평가(Evaluation): 성능을 평가하고 조정한다.

---

# Transfer Learning이란?

Transfer Learning(전이 학습)은 한 도메인에서 학습한 지식을 다른 도메인에 적용하는 기법으로, NLP 및 컴퓨터 비전에서 널리 사용된다.

## 1. 출현 배경
기존 기계 학습 모델은 특정 데이터셋에서만 학습되었으므로 새로운 데이터에서 성능이 저하되는 문제가 있었다. 이를 해결하기 위해 대규모 데이터에서 사전 학습(Pre-training) 후, 특정 태스크에 미세 조정(Fine-tuning)하는 방법이 등장했다.

## 2. 단어 임베딩의 한계
기존의 Word2Vec, GloVe 등 단어 임베딩 기법은 문맥 정보를 반영하지 못한다. 같은 단어라도 문맥에 따라 의미가 달라질 수 있는데, 정적인 임베딩 방식은 이를 반영하지 못한다.

## 3. Transfer Learning in CV
컴퓨터 비전에서는 ImageNet에서 사전 학습된 모델을 다양한 태스크에 적용하는 방식이 보편적이다.

## 4. Pre-training & Fine-tuning

- Pre-training: 대규모 데이터에서 일반적인 언어 구조를 학습한다.
- Fine-tuning: 특정 태스크에 맞게 추가 학습을 진행한다.

### Transfer Learning의 효율성
- 데이터가 적어도 높은 성능을 달성 가능
- 연산량 감소
- 기존의 모델보다 빠른 학습 속도

---

# ELMo (Embeddings from Language Models)

ELMo는 문맥을 반영한 단어 임베딩 기법으로, LSTM 기반 언어 모델을 활용한다.

## 1. 모델 구조
- 양방향 LSTM(BiLSTM) 사용
- 다층 언어 모델 적용

## 2. Pre-training
- 대량의 텍스트에서 학습
- 문맥을 고려하여 단어 임베딩을 생성

## 3. Contextualized Word Embedding
- 같은 단어라도 문맥에 따라 다른 벡터 표현을 가짐

## 4. 기존 단어 임베딩과 ELMo
- Word2Vec, GloVe는 단어별로 고정된 벡터
- ELMo는 문맥에 따라 동적으로 변화하는 벡터 제공

## 5. Pretrained Language Model (PLM)
- 미리 학습된 모델을 다양한 NLP 태스크에 활용 가능

## 6. 보완해야 할 점
- 계산량이 많고 느림
- Transformer 기반 모델(BERT)의 등장으로 대체됨

---

# BERT (Bidirectional Encoder Representations from Transformers)

## 1. BERT의 구조
- Transformer 기반의 양방향 언어 모델
- 모든 문맥을 동시에 고려하여 더 강력한 언어 표현 학습

## 2. ELMo와 BERT
- ELMo는 LSTM 기반, BERT는 Transformer 기반
- BERT는 더 깊은 문맥 정보를 학습 가능

## 3. 두 가지 버전
- BERT-Base: 12-layer, 768 hidden size
- BERT-Large: 24-layer, 1024 hidden size

## 4. Pretraining
- Masked Language Model (MLM): 문장의 일부 단어를 마스킹하여 예측하는 방식
- Next Sentence Prediction (NSP): 두 문장이 연속적인지 예측

## 5. Downstream Tasks
- Fine-tuning
- Single Sentence Classification
- Sentence Pair Classification
- Question Answering
- Single Sentence Tagging

## 6. BERT 단어 임베딩의 활용
- Feature Extraction

---

# Tokenizer 이해하기

## 1. Tokenizer 개념
- Tokenization: 문장을 토큰 단위로 분리하는 과정
- Subword Tokenizer: 단어보다 작은 단위까지 분해하는 기법

### 2. BPE 알고리즘
- Byte Pair Encoding (BPE)
- 자주 등장하는 문자 쌍을 결합하여 새로운 단위를 생성

# Decoder Model (GPT-1)

## GPT (Generative Pre-Training)
GPT는 "Generative Pre-trained Transformer"의 약자로, OpenAI에서 개발한 자연어 처리(NLP) 모델입니다. 이 모델은 두 단계의 학습 과정으로 이루어집니다.

1. Pre-training (사전 학습): 방대한 양의 텍스트 데이터를 사용하여 언어 모델을 학습합니다. 이를 통해 문장의 구조와 문맥을 파악할 수 있도록 합니다.
2. Fine-tuning (미세 조정): 특정 태스크(예: 문장 분류, 질의 응답 등)에 맞게 추가적으로 학습을 진행합니다.

GPT-1은 Transformer의 Decoder 구조를 기반으로 하며, 순차적인 데이터를 예측하는 방식으로 학습됩니다.

## Self-Attention vs Masked Self-Attention
Transformer 모델에서 Self-Attention과 Masked Self-Attention의 차이점은 다음과 같습니다.

- Self-Attention: 문장 내의 모든 단어가 서로를 참조할 수 있도록 하는 메커니즘입니다. 이를 통해 문맥을 파악하고, 멀리 떨어진 단어 간의 관계도 학습할 수 있습니다.
- Masked Self-Attention: GPT 모델에서는 예측할 단어 이후의 정보가 보이지 않도록 마스킹(masking)을 적용합니다. 이는 모델이 정답을 미리 알지 않고, 앞의 단어만으로 다음 단어를 예측하게 만듭니다.

## Pre-training
GPT-1의 사전 학습 과정은 언어 모델링(Language Modeling)을 중심으로 진행됩니다.
- 대량의 비지도 학습 데이터(예: 위키백과, 웹 문서)를 활용하여 문맥을 학습합니다.
- 주어진 문장에서 다음 단어를 예측하는 방식(Autoregressive Language Modeling)을 사용합니다.

## Fine-tuning
사전 학습이 완료된 모델을 특정 태스크에 맞게 조정하는 과정입니다.
- 태스크별 레이블이 있는 데이터셋을 사용하여 supervised learning 방식으로 추가 학습합니다.
- 예를 들어, 감성 분석, 텍스트 요약, 번역 등의 작업을 수행할 수 있습니다.

## Downstream Tasks
GPT 모델은 다양한 NLP 태스크에서 활용됩니다. 대표적인 태스크는 다음과 같습니다.

### Single Sentence Classification
- 감성 분석(Sentiment Analysis), 주제 분류(Topic Classification) 등에서 사용됩니다.

### Textual Entailment
- 두 문장이 논리적으로 연결되는지 판별하는 작업입니다.

### Similarity
- 두 문장이 의미적으로 유사한지 비교하는 태스크입니다.

### Question Answering and Commonsense Reasoning
- 질의응답(Task: QA) 및 상식적 추론(Commonsense Reasoning) 문제를 해결하는 데 사용됩니다.

---

# Decoder Model (GPT-2, GPT-3) 이해하기

## GPT-1 vs GPT-2 vs GPT-3
GPT 모델은 발전하면서 다음과 같은 변화가 있었습니다.

| 모델  | 주요 특징 |
|--------|------------|
| GPT-1  | 1.17억 개의 파라미터, 제한된 데이터셋으로 학습됨 |
| GPT-2  | 15억 개의 파라미터, 훨씬 더 많은 데이터셋으로 학습됨 |
| GPT-3  | 1750억 개의 파라미터, Few-Shot Learning 개념 도입 |

## GPT-2, GPT-3 공통점
- Transformer 기반의 Decoder-only 모델
- 비지도 학습을 통해 대규모 코퍼스로 Pre-training 진행
- Autoregressive Language Modeling 방식 사용
- Zero-shot, One-shot, Few-shot Learning 방식 적용 가능

## GPT-2 이전 모델의 한계
- GPT-1은 상대적으로 적은 양의 데이터와 파라미터로 학습되어 일반화 성능이 낮았음
- 특정 태스크에 맞게 Fine-tuning해야 하는 단점 존재

## GPT-2 (Language Models are Unsupervised Multi-task Learners)
GPT-2의 핵심 개념은 비지도 학습을 통해 다양한 NLP 태스크를 수행할 수 있다는 것입니다.

- 사전 학습 데이터 크기가 대폭 증가하고, 모델 크기도 커짐
- Fine-tuning 없이도 다양한 태스크를 수행할 수 있도록 설계됨
- Zero-shot Learning 성능이 향상됨

## GPT-3 (Language Models are Few-Shot Learners)
GPT-3는 이전 모델보다 100배 이상 많은 파라미터(1750억 개)를 사용하여 학습되었습니다.

- Few-shot Learning을 통해 태스크별 학습 데이터 없이도 높은 성능을 달성
- GPT-3는 훈련 없이 예제만 제공해도 문제 해결이 가능함 (예: 번역, 코드 생성 등)
- API 기반으로 대중에게 제공되어 다양한 애플리케이션에서 활용됨

## 모델이 자연어 처리 Task를 해결하는 방식
GPT-2와 GPT-3는 텍스트 생성(text generation) 방식을 사용하여 다양한 태스크를 해결합니다.

- Prompt Engineering: 적절한 프롬프트(입력)를 주면 GPT가 원하는 답을 생성할 수 있도록 유도
- Zero-shot, One-shot, Few-shot Learning 방식 지원
  - Zero-shot: 태스크에 대한 설명만 제공하고 해결
  - One-shot: 예제 하나 제공 후 해결
  - Few-shot: 예제 몇 개 제공 후 해결

## GPT-2, GPT-3 한계
1. 긴 문서의 문맥 유지 어려움:
   - GPT-2, GPT-3는 제한된 context window(입력 길이)로 인해 긴 문서에서 문맥을 유지하는 데 어려움이 있음.
2. 계산 비용이 큼:
   - GPT-3는 1750억 개의 파라미터를 가지고 있어 학습 및 추론 비용이 매우 높음.
3. 사전 학습된 데이터 편향:
   - 모델이 학습한 데이터가 편향되어 있을 경우, 부적절한 답변을 생성할 가능성이 있음.
4. 추론 가능성 부족:
   - GPT 계열 모델은 기존 데이터에서 패턴을 학습하여 답변을 생성하지만, 논리적 추론이나 사실 검증이 어려움.

---


# 5-1



